100 Days of Deep Learning | Course Announcement
What is Deep Learning? Deep Learning Vs Machine Learning | Complete Deep Learning Course
Types of Neural Networks | History of Deep Learning | Applications of Deep Learning
What is a Perceptron? Perceptron Vs Neuron | Perceptron Geometric Intuition
Perceptron Trick | How to train a Perceptron | Perceptron Part 2 | Deep Learning Full Course
Perceptron Loss Function | Hinge Loss | Binary Cross Entropy | Sigmoid Function
Problem with Perceptron
MLP Notation
Multi Layer Perceptron | MLP Intuition
Forward Propagation | How a neural network predicts output?
Customer Churn Prediction using ANN | Keras and Tensorflow | Deep Learning Classification
Handwritten Digit Classification using ANN | MNIST Dataset
Graduate Admission Prediction using ANN
Loss Functions in Deep Learning | Deep Learning | CampusX
Backpropagation in Deep Learning | Part 1 | The What?
Backpropagation Part 2 | The How | Complete Deep Learning Playlist
Backpropagation Part 3 | The Why | Complete Deep Learning Playlist
MLP Memoization | Complete Deep Learning Playlist
Gradient Descent in Neural Networks | Batch vs Stochastics vs Mini Batch Gradient Descent
Vanishing Gradient Problem in ANN | Exploding Gradient Problem | Code Example
How to Improve the Performance of a Neural Network
Early Stopping In Neural Networks | End to End Deep Learning Course
Data Scaling in Neural Network | Feature Scaling in ANN | End to End Deep Learning Course
Dropout Layer in Deep Learning | Dropouts in ANN | End to End Deep Learning
Dropout Layers in ANN | Code Example | Regression | Classification
Regularization in Deep Learning | L2 Regularization in ANN | L1 Regularization | Weight Decay in ANN
Activation Functions in Deep Learning | Sigmoid, Tanh and Relu Activation Function
Relu Variants Explained | Leaky Relu | Parametric Relu | Elu | Selu | Activation Functions Part 2
Weight Initialization Techniques | What not to do? | Deep Learning
Xavier/Glorat And He Weight Initialization in Deep Learning
Batch Normalization in Deep Learning | Batch Learning in Keras
Optimizers in Deep Learning | Part 1 | Complete Deep Learning Course
Exponentially Weighted Moving Average or Exponential Weighted Average | Deep Learning
SGD with Momentum Explained in Detail with Animations | Optimizers in Deep Learning Part 2
Nesterov Accelerated Gradient (NAG) Explained in Detail | Animations | Optimizers in Deep Learning
AdaGrad Explained in Detail with Animations | Optimizers in Deep Learning Part 4
RMSProp Explained in Detail with Animations | Optimizers in Deep Learning Part 5
Adam Optimizer Explained in Detail with Animations | Optimizers in Deep Learning Part 5
Keras Tuner | Hyperparameter Tuning a Neural Network
What is Convolutional Neural Network (CNN) | CNN Intution
CNN Vs Visual Cortex | The Famous Cat Experiment | History of CNN
CNN Part 3 | Convolution Operation
Padding & Strides in CNN | CNN Lecture 4 | Deep Learning
Pooling Layer in CNN | MaxPooling in Convolutional Neural Network
CNN Architecture | LeNet -5 Architecture
Comparing CNN Vs ANN | CampusX
Backpropagation in CNN | Part 1 | Deep Learning
CNN Backpropagation Part 2 | How Backpropagation works on Convolution, Maxpooling and Flatten Layers
Cat Vs Dog Image Classification Project | Deep Learning Project | CNN Project
Data Augmentation in Deep Learning | CNN
Pretrained models in CNN | ImageNET Dataset | ILSVRC | Keras Code
What does a CNN see? | Visualizing CNN Filters and Feature Maps | CampusX
What is Transfer Learning? Transfer Learning in Keras | Fine Tuning Vs Feature Extraction
Keras Functional Model | How to build non-linear Neural Networks?
Why RNNs are needed | RNNs Vs ANNs | RNN Part 1
Recurrent Neural Network | Forward Propagation | Architecture
RNN Sentiment Analysis | RNN Code Example in Keras | CampusX
Types of RNN | Many to Many | One to Many | Many to One RNNs
How Backpropagation works in RNN | Backpropagation Through Time
Problems with RNN | 100 Days of Deep Learning
LSTM | Long Short Term Memory | Part 1 | The What? | CampusX
LSTM Architecture | Part 2 | The How? | CampusX
LSTM | Part 3 | Next Word Predictor Using | CampusX
Gated Recurrent Unit | Deep Learning | GRU | CampusX
Deep RNNs | Stacked RNNs | Stacked LSTMs | Stacked GRUs | CampusX
Bidirectional RNN | BiLSTM | Bidirectional LSTM | Bidirectional GRU
The Epic History of Large Language Models (LLMs) | From LSTMs to ChatGPT | CampusX
Encoder Decoder | Sequence-to-Sequence Architecture | Deep Learning | CampusX
Attention Mechanism in 1 video | Seq2Seq Networks | Encoder Decoder Architecture
Bahdanau Attention Vs Luong Attention
Introduction to Transformers | Transformers Part 1
What is Self Attention | Transformers Part 2 | CampusX
Self Attention in Transformers | Deep Learning | Simple Explanation with Code!
Scaled Dot Product Attention | Why do we scale Self Attention?
Self Attention Geometric Intuition | How to Visualize Self Attention | CampusX
Why is Self Attention called "Self"? | Self Attention Vs Luong Attention in Depth Lecture | CampusX
What is Multi-head Attention in Transformers | Multi-head Attention v Self Attention | Deep Learning
